{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d79bd40",
   "metadata": {},
   "source": [
    "Реализация triton (cuda если интересно) кернелей для квантизации весов в LLM и инференса квантизованной модели.\n",
    "\n",
    "План:\n",
    "1) Реализовать кернель для квантизации 2D матрицы из fp16 в int4\n",
    "и последующей упаковки квантизованной матрицы в int8 или int32.\n",
    "При этом потребляемая память должна уменьшиться в 4 раза.\n",
    "2) Реализовать кернель для перемножения матрицы в bf16 на квантизованную матрицу в int4 на (X16@W4^T)\n",
    "3) Сравнить скорость перемножения (X16@W4^T) с (X16@W16^T). Размеры матрицы W такие же, как размеры матриц весов для модели Llama-3.2-1B-Instruct (https://huggingface.co/unsloth/Llama-3.2-1B-Instruct).\n",
    "Количество строк (токенов) в матрице активаций X: 128, 512, 2048\n",
    "4) С использованием написанных кернелей написать квантизованный линейный слой и применить его к линейныс слоям модели Llama-3.2-1B-Instruct\n",
    "5) Замерить скорость расчета и уровень перплексии на wikitext2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2005301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59db856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e16b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 4714.26\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_model_size_mb(model: torch.nn.Module) -> float:\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    return (param_size + buffer_size) / (1024 ** 2)\n",
    "\n",
    "size_mb = get_model_size_mb(model)\n",
    "print(f\"Model size (MB): {size_mb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de3ab697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d879fd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 2048])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj.weight.shape\n",
    "# model.model.layers[0].mlp.gate_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a21eacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feda43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# path = \"/kaggle/working/gate_proj_weight_0.pt\"\n",
    "# torch.save(model.model.layers[0].mlp.gate_proj.weight, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2839bee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2aeb61f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0260, device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46be65bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0260, -0.0123, -0.0140,  ...,  0.0035,  0.0272,  0.0364],\n",
       "        [-0.0096,  0.0225,  0.0015,  ..., -0.0272, -0.0151,  0.0024],\n",
       "        [-0.0261, -0.0095,  0.0002,  ..., -0.0056, -0.0120, -0.0287],\n",
       "        ...,\n",
       "        [ 0.0198, -0.0228, -0.0190,  ...,  0.0289, -0.0427, -0.0137],\n",
       "        [-0.0026, -0.0109,  0.0011,  ..., -0.0037,  0.0061, -0.0140],\n",
       "        [-0.0231,  0.0049,  0.0086,  ..., -0.0120, -0.0069,  0.0070]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23a35b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0260, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj.weight.to(torch.float16)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c69b5810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6c3d7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=4),\n",
    "        triton.Config({\"BLOCK_SIZE\": 2048}, num_stages=1),\n",
    "    ],\n",
    "    key=[\"n_elements\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _quantize_rowwise(x_ptr, output_ptr, output_maxs, n_elements, BLOCK_SIZE: tl.constexpr, P2: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * n_elements\n",
    "    row_start_ptr = x_ptr + block_start\n",
    "\n",
    "    idx = tl.arange(0, P2 // 2)\n",
    "    off_even = 2 * idx\n",
    "    off_odd = 2 * idx + 1\n",
    "\n",
    "    mask_even = off_even < n_elements\n",
    "    mask_odd = off_odd < n_elements\n",
    "\n",
    "    x_even = tl.load(row_start_ptr + off_even, mask=mask_even, other=0.0)\n",
    "    x_odd = tl.load(row_start_ptr + off_odd, mask=mask_odd, other=0.0)\n",
    "\n",
    "    absmax_even = tl.max(tl.abs(x_even))\n",
    "    absmax_odd = tl.max(tl.abs(x_odd))\n",
    "    absmax = tl.maximum(absmax_even, absmax_odd)\n",
    "\n",
    "    scale = tl.where(absmax == 0, 0.0, 7.0 / absmax)\n",
    "\n",
    "    s_even = x_even * scale\n",
    "    s_odd = x_odd * scale\n",
    "\n",
    "    q_even = tl.where(s_even >= 0, s_even + 0.5, s_even - 0.5).to(tl.int8).to(tl.uint8) & 0xF\n",
    "    q_odd = tl.where(s_odd >= 0, s_odd + 0.5, s_odd - 0.5).to(tl.int8).to(tl.uint8) & 0xF\n",
    "\n",
    "    packed = (q_odd << 4) | q_even\n",
    "\n",
    "    packed_block_start = pid * ((n_elements + 1) // 2)\n",
    "    packed_mask = idx < ((n_elements + 1) // 2)\n",
    "\n",
    "    tl.store(output_ptr + packed_block_start + idx, packed, mask=packed_mask)\n",
    "    tl.store(output_maxs + pid, absmax)\n",
    "\n",
    "\n",
    "def quantize_rowwise(x: torch.Tensor):\n",
    "    N = x.shape[0]\n",
    "    M = x.shape[1]\n",
    "\n",
    "    out_cols = (M + 1) // 2\n",
    "\n",
    "    output_tensor = torch.empty((N, out_cols), dtype=torch.uint8, device=x.device)\n",
    "\n",
    "    output_maxs = torch.empty(N, dtype=torch.float16, device=x.device)\n",
    "\n",
    "    P2 = 2 ** int(torch.ceil(torch.log2(torch.tensor(M, dtype=torch.float16))))\n",
    "\n",
    "    grid = lambda meta: (N,)\n",
    "    _quantize_rowwise[grid](x_ptr=x, output_ptr=output_tensor, output_maxs=output_maxs, n_elements=M, P2=P2)\n",
    "\n",
    "    return output_tensor, output_maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2dac065",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_quant, max_values = quantize_rowwise(model.model.layers[0].mlp.gate_proj.weight.to('cuda').to(torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.layers[0].mlp.gate_proj.weight = torch.nn.Parameter(weight_quant, requires_grad=False)\n",
    "# model.model.layers[0].mlp.gate_proj.max_values = max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98eabc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 64,  \"BLOCK_K\": 64},  num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64,  \"BLOCK_K\": 64},  num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 128, \"BLOCK_K\": 64},  num_warps=4, num_stages=2)\n",
    "    ],\n",
    "    key=[\"B\", \"IN\", \"OUT\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _forward_int4_fused_kernel(x_q_ptr,\n",
    "                               w_q_ptr, w_scale_ptr,\n",
    "                               b_ptr, y_ptr,\n",
    "                               B, IN, OUT,\n",
    "                               BLOCK_M: tl.constexpr,\n",
    "                               BLOCK_N: tl.constexpr,\n",
    "                               BLOCK_K: tl.constexpr,\n",
    "                               PER_CHANNEL: tl.constexpr,\n",
    "                               HAS_BIAS: tl.constexpr):\n",
    "    pid_0 = tl.program_id(0)\n",
    "    pid_1 = tl.program_id(1)\n",
    "\n",
    "    acc = tl.full((BLOCK_M, BLOCK_N), 0.0, dtype=tl.float32)\n",
    "\n",
    "    pid_0_off = (tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) * OUT\n",
    "    pid_1_off = tl.arange(0, BLOCK_N) + pid_1 * BLOCK_N\n",
    "    off = pid_0_off[:, None] + pid_1_off[None, :]\n",
    "    \n",
    "    out_mask = ((tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) < B)[:, None] & \\\n",
    "               (pid_1_off < OUT)[None, :]  \n",
    "\n",
    "    for k in range(0, IN, BLOCK_K):\n",
    "        off_x_d0 = (tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) * IN\n",
    "        off_x_d1 = (tl.arange(0, BLOCK_K) + k)\n",
    "        off_x = off_x_d0[:, None] + off_x_d1[None, :]\n",
    "        mask_x = (off_x_d1 < IN)[None, :] & ((tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) < B)[:, None]\n",
    "\n",
    "        packed_IN = (IN + 1) // 2\n",
    "        global_cols = pid_1 * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "        out_guard = global_cols < OUT\n",
    "        safe_cols = tl.where(out_guard, global_cols, 0)\n",
    "        k_indices = tl.arange(0, BLOCK_K) + k\n",
    "        row_offsets = safe_cols[None, :] * packed_IN\n",
    "        byte_cols = (k_indices // 2)[:, None]\n",
    "        off_w = row_offsets + byte_cols\n",
    "        mask_w = (k_indices[:, None] < IN) & out_guard[None, :]\n",
    "        is_high = (k_indices & 1) == 1\n",
    "        \n",
    "\n",
    "        x = tl.load(x_q_ptr + off_x, mask_x, 0)\n",
    "        w_byte = tl.load(w_q_ptr + off_w, mask_w, 0)\n",
    "\n",
    "        w_u32 = w_byte.to(tl.uint32)\n",
    "        low = w_u32 & 0xF\n",
    "        high = (w_u32 >> 4) & 0xF\n",
    "        sel = is_high[:, None]\n",
    "        w_nib = tl.where(sel, high, low)\n",
    "        w_i32 = w_nib.to(tl.int32)\n",
    "        w_signed_i32 = tl.where(w_i32 < 8, w_i32, w_i32 - 16)\n",
    "\n",
    "        x_f16 = x.to(tl.float16)\n",
    "        w_f16 = w_signed_i32.to(tl.float16)\n",
    "        acc += tl.dot(x_f16, w_f16)\n",
    "    \n",
    "        \n",
    "\n",
    "    if PER_CHANNEL:\n",
    "        w_scale_mask = pid_1_off < OUT\n",
    "        w_scale = tl.load(w_scale_ptr + pid_1_off, mask=w_scale_mask)\n",
    "        alpha = w_scale[None, :].to(tl.float32)\n",
    "    else:\n",
    "        w_scale = tl.load(w_scale_ptr)\n",
    "        alpha = w_scale.to(tl.float32)\n",
    "\n",
    "    if HAS_BIAS:\n",
    "        bias_mask = pid_1_off < OUT\n",
    "        bias = tl.load(b_ptr + pid_1_off, mask=bias_mask, other=0).to(tl.float32)\n",
    "        acc = acc * alpha + bias[None, :]\n",
    "    else:\n",
    "        acc = acc * alpha\n",
    "\n",
    "   \n",
    "    tl.store(y_ptr + off, acc.to(tl.float16), out_mask)               \n",
    "\n",
    "def matmul_int4_fused(x: torch.Tensor,\n",
    "                      w_q: torch.Tensor,\n",
    "                      w_scale: torch.Tensor,\n",
    "                      bias: torch.Tensor | None = None,\n",
    "                      *, per_channel: bool = True) -> torch.Tensor:\n",
    "\n",
    "    B, IN = x.shape\n",
    "    OUT = w_scale.shape[0]\n",
    "\n",
    "    x_f16 = x.to(torch.float16)\n",
    "    w_scale_f16 = (w_scale.to(dtype=torch.float16, device=x.device) / 7)\n",
    "    y = torch.empty((B, OUT), dtype=torch.float16, device=x.device)\n",
    "\n",
    "    grid = lambda meta: (triton.cdiv(B, meta[\"BLOCK_M\"]),\n",
    "                     triton.cdiv(OUT, meta[\"BLOCK_N\"]))\n",
    "\n",
    "    _forward_int4_fused_kernel[grid](x_q_ptr=x_f16,\n",
    "                               w_q_ptr=w_q, w_scale_ptr=w_scale_f16,\n",
    "                               b_ptr=bias, y_ptr=y,\n",
    "                               B=B, IN=IN, OUT=OUT,\n",
    "                               PER_CHANNEL=per_channel,\n",
    "                               HAS_BIAS=(bias is not None))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b62ab52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, weight_quant: torch.Tensor, weight_scale: torch.Tensor, bias: torch.Tensor | None = None):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_q = weight_quant\n",
    "        self.weight_scale = weight_scale\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        orig_shape = x.shape\n",
    "        x_2d = x.reshape(-1, orig_shape[-1])\n",
    "        y_2d = matmul_int4_fused(\n",
    "            x=x_2d,\n",
    "            w_q=self.weight_q,\n",
    "            w_scale=self.weight_scale,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "        y = y_2d.reshape(*orig_shape[:-1], self.out_features)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20cf0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "\n",
    "def quantize_linear_module(linear: torch.nn.Linear) -> QuantLinear:\n",
    "    in_features = linear.in_features\n",
    "    out_features = linear.out_features\n",
    "    weight = linear.weight.detach().to(device=device, dtype=torch.float16)\n",
    "    weight_q, max_values = quantize_rowwise(weight)\n",
    "    bias = None\n",
    "    if linear.bias is not None:\n",
    "        bias = linear.bias.detach().to(device=device, dtype=torch.float16)\n",
    "    return QuantLinear(in_features, out_features, weight_q, max_values, bias)\n",
    "\n",
    "def replace_linear_with_quantlinear(module: torch.nn.Module):\n",
    "    for child_name, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            quant_linear = quantize_linear_module(child)\n",
    "            setattr(module, child_name, quant_linear)\n",
    "        else:\n",
    "            replace_linear_with_quantlinear(child)\n",
    "\n",
    "replace_linear_with_quantlinear(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70b55552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0957, 0.0957, 0.0645,  ..., 0.0767, 0.1133, 0.0591], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[5].mlp.gate_proj.weight_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.layers[0].mlp.gate_proj = QuantLinear(2048, 8192, model.model.layers[0].mlp.gate_proj.weight, model.model.layers[0].mlp.gate_proj.max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e54a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence (AI) called \"Luna\" (or \"Luni\" for short). I'm an artificial intelligence designed to simulate conversation, answer questions, and even provide information and\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "74e0986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB): 1002.26\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_model_size_mb(model: torch.nn.Module) -> float:\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    return (param_size + buffer_size) / (1024 ** 2)\n",
    "\n",
    "size_mb = get_model_size_mb(model)\n",
    "print(f\"Model size (MB): {size_mb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54abbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b9d4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=False)\n",
    "\n",
    "tokenized = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "block_size = 2048\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = (len(concatenated[\"input_ids\"]) // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized.map(group_texts, batched=True)\n",
    "\n",
    "lm_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6acbc61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 71.0608\n",
      "Eval time (s): 143.44\n",
      "Tokens processed: 249856\n",
      "Tokens/sec: 1741.88\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import math\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloader = DataLoader(lm_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "start_time = time.time()\n",
    "num_tokens = 0\n",
    "loss_sum = 0.0\n",
    "count = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch.get(\"attention_mask\"),\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        num_tokens += batch[\"labels\"].numel()\n",
    "        loss_sum += loss.item()\n",
    "        count += 1\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "elapsed = time.time() - start_time\n",
    "mean_loss = loss_sum / max(count, 1)\n",
    "ppl = math.exp(mean_loss)\n",
    "tps = num_tokens / elapsed if elapsed > 0 else float(\"nan\")\n",
    "\n",
    "print(f\"Perplexity: {ppl:.4f}\")\n",
    "print(f\"Eval time (s): {elapsed:.2f}\")\n",
    "print(f\"Tokens processed: {num_tokens}\")\n",
    "print(f\"Tokens/sec: {tps:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71eadbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "03d1b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some tips on how to hire a good engineer:\n",
      "\n",
      "1.  **Define the Job Requirements**: Before you can start interviewing engineers, you need to define their requirements. Consider factors like:\n",
      "    * Education and experience (Bachelor's or Master's degree, typically 1-2 years, and relevant work experience)\n",
      "    * Language (in English or a language) and skills (in computer programming or other relevant technical skills)\n",
      "    * Technical skills (software or hardware relevant skills, such as:\n",
      "        * Database design (SQL, MySQL, or Oracle)\n",
      "        * Data visualization (Table, Chart, and Graph)\n",
      "        * Data mining (SQL, MySQL, or Oracle)\n",
      "    * Software or hardware relevant skills (e.g., SQL Server, Oracle Database)\n",
      "    * Domain (web development, e.g., PHP, Java, or.NET)\n",
      "    * Database (database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design, database design,\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"how to hire a good engineer?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
