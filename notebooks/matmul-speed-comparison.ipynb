{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6c27cd",
   "metadata": {},
   "source": [
    "3) Сравнить скорость перемножения (X16@W4^T) с (X16@W16^T). Размеры матрицы W такие же, как размеры матриц весов для модели Llama-3.2-1B-Instruct (https://huggingface.co/unsloth/Llama-3.2-1B-Instruct).\n",
    "Количество строк (токенов) в матрице активаций X: 128, 512, 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef8374a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "GPU Model: Tesla T4\n",
      "CUDA Version: 12.4\n",
      "Number of GPUs: 2\n",
      "Current GPU: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check CUDA availability and GPU model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available\")\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d589162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 128, \"BLOCK_K\": 32},  num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 128, \"BLOCK_K\": 64},  num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32},  num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 64},  num_warps=4, num_stages=3),\n",
    "\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 256, \"BLOCK_K\": 32},  num_warps=4, num_stages=2),\n",
    "        triton.Config({\"BLOCK_M\": 64,  \"BLOCK_N\": 256, \"BLOCK_K\": 64},  num_warps=4, num_stages=3),\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 32},  num_warps=4, num_stages=3),\n",
    "        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 64},  num_warps=8, num_stages=3),\n",
    "    ],\n",
    "    key=[\"B\", \"IN\", \"OUT\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _forward_int4_fused_kernel(x_q_ptr,\n",
    "                               w_q_ptr, w_scale_ptr,\n",
    "                               b_ptr, y_ptr,\n",
    "                               B, IN, OUT,\n",
    "                               BLOCK_M: tl.constexpr,\n",
    "                               BLOCK_N: tl.constexpr,\n",
    "                               BLOCK_K: tl.constexpr,\n",
    "                               PER_CHANNEL: tl.constexpr,\n",
    "                               HAS_BIAS: tl.constexpr):\n",
    "    pid_0 = tl.program_id(0)\n",
    "    pid_1 = tl.program_id(1)\n",
    "\n",
    "    acc = tl.full((BLOCK_M, BLOCK_N), 0.0, dtype=tl.float32)\n",
    "\n",
    "    pid_0_off = (tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) * OUT\n",
    "    pid_1_off = tl.arange(0, BLOCK_N) + pid_1 * BLOCK_N\n",
    "    off = pid_0_off[:, None] + pid_1_off[None, :]\n",
    "    \n",
    "    out_mask = ((tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) < B)[:, None] & \\\n",
    "               (pid_1_off < OUT)[None, :]  \n",
    "\n",
    "    for k in range(0, IN, BLOCK_K):\n",
    "        off_x_d0 = (tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) * IN\n",
    "        off_x_d1 = (tl.arange(0, BLOCK_K) + k)\n",
    "        off_x = off_x_d0[:, None] + off_x_d1[None, :]\n",
    "        mask_x = (off_x_d1 < IN)[None, :] & ((tl.arange(0, BLOCK_M) + pid_0 * BLOCK_M) < B)[:, None]\n",
    "\n",
    "        packed_IN = (IN + 1) // 2\n",
    "        global_cols = pid_1 * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "        out_guard = global_cols < OUT\n",
    "        safe_cols = tl.where(out_guard, global_cols, 0)\n",
    "        k_indices = tl.arange(0, BLOCK_K) + k\n",
    "        row_offsets = safe_cols[None, :] * packed_IN\n",
    "        byte_cols = (k_indices // 2)[:, None]\n",
    "        off_w = row_offsets + byte_cols\n",
    "        mask_w = (k_indices[:, None] < IN) & out_guard[None, :]\n",
    "        is_high = (k_indices & 1) == 1\n",
    "        \n",
    "\n",
    "        x = tl.load(x_q_ptr + off_x, mask_x, 0)\n",
    "        w_byte = tl.load(w_q_ptr + off_w, mask_w, 0)\n",
    "\n",
    "        w_u32 = w_byte.to(tl.uint32)\n",
    "        low = w_u32 & 0xF\n",
    "        high = (w_u32 >> 4) & 0xF\n",
    "        sel = is_high[:, None]\n",
    "        w_nib = tl.where(sel, high, low)\n",
    "        w_i32 = w_nib.to(tl.int32)\n",
    "        w_signed_i32 = tl.where(w_i32 < 8, w_i32, w_i32 - 16)\n",
    "\n",
    "        x_f16 = x.to(tl.float16)\n",
    "        w_f16 = w_signed_i32.to(tl.float16)\n",
    "        acc += tl.dot(x_f16, w_f16)\n",
    "    \n",
    "        \n",
    "\n",
    "    if PER_CHANNEL:\n",
    "        w_scale_mask = pid_1_off < OUT\n",
    "        w_scale = tl.load(w_scale_ptr + pid_1_off, mask=w_scale_mask)\n",
    "        alpha = w_scale[None, :].to(tl.float32)\n",
    "    else:\n",
    "        w_scale = tl.load(w_scale_ptr)\n",
    "        alpha = w_scale.to(tl.float32)\n",
    "\n",
    "    if HAS_BIAS:\n",
    "        bias_mask = pid_1_off < OUT\n",
    "        bias = tl.load(b_ptr + pid_1_off, mask=bias_mask, other=0).to(tl.float32)\n",
    "        acc = acc * alpha + bias[None, :]\n",
    "    else:\n",
    "        acc = acc * alpha\n",
    "\n",
    "   \n",
    "    tl.store(y_ptr + off, acc.to(tl.float16), out_mask)               \n",
    "\n",
    "def matmul_int4_fused(x: torch.Tensor,\n",
    "                      w_q: torch.Tensor,\n",
    "                      w_scale: torch.Tensor,\n",
    "                      bias: torch.Tensor | None = None,\n",
    "                      *, per_channel: bool = True) -> torch.Tensor:\n",
    "\n",
    "    B, IN = x.shape\n",
    "    OUT = w_scale.shape[0]\n",
    "\n",
    "    x_f16 = x.to(torch.float16)\n",
    "    w_scale_f16 = (w_scale.to(dtype=torch.float16, device=x.device) / 7)\n",
    "    y = torch.empty((B, OUT), dtype=torch.float16, device=x.device)\n",
    "\n",
    "    grid = lambda meta: (triton.cdiv(B, meta[\"BLOCK_M\"]),\n",
    "                     triton.cdiv(OUT, meta[\"BLOCK_N\"]))\n",
    "\n",
    "    _forward_int4_fused_kernel[grid](x_q_ptr=x_f16,\n",
    "                               w_q_ptr=w_q, w_scale_ptr=w_scale_f16,\n",
    "                               b_ptr=bias, y_ptr=y,\n",
    "                               B=B, IN=IN, OUT=OUT,\n",
    "                               PER_CHANNEL=per_channel,\n",
    "                               HAS_BIAS=(bias is not None))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be43017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512, 2048, dtype=torch.float16, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee46f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/kaggle/working/gate_proj_weight_0_quant.pt\"\n",
    "w_quant = torch.load(path)\n",
    "\n",
    "\n",
    "path = \"/kaggle/working/gate_proj_weight_0_quant_scale.pt\"\n",
    "w_scale = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8871383",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = matmul_int4_fused(x, w_quant, w_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ec45aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = matmul_int4_fused(x, w_quant, w_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d367a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/kaggle/working/gate_proj_weight_0.pt\"\n",
    "w = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f303718",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77fe2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3ead608",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wo_q = x @ w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "093f3e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3804, -0.3999, -0.5967,  ..., -1.2314, -0.8862,  0.3347],\n",
       "        [ 1.5898,  0.2499,  0.1747,  ...,  2.7715,  1.2373, -1.1924],\n",
       "        [ 1.8408,  0.3977, -1.5039,  ..., -0.1364, -1.0596,  0.0104],\n",
       "        ...,\n",
       "        [-1.4541, -0.3726,  1.3721,  ...,  1.3975, -0.3584, -1.9141],\n",
       "        [ 0.5132, -1.2920,  0.0789,  ...,  0.5181, -1.5557, -0.7412],\n",
       "        [-0.2737,  0.7788, -0.0873,  ..., -1.3936,  1.9121, -1.2314]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83883091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5435, -0.5117, -0.2964,  ..., -1.3330, -0.7231,  0.2429],\n",
       "        [ 1.5557,  0.2181,  0.2837,  ...,  2.7109,  1.2432, -1.0713],\n",
       "        [ 1.6670,  0.1923, -1.6064,  ..., -0.3875, -1.0605,  0.0747],\n",
       "        ...,\n",
       "        [-1.3945, -0.3428,  1.4385,  ...,  1.3604, -0.1771, -1.9512],\n",
       "        [ 0.6309, -1.1504,  0.1685,  ...,  0.2430, -1.5176, -0.7949],\n",
       "        [-0.3479,  0.6191,  0.2129,  ..., -1.2236,  1.9395, -1.2617]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_wo_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "391766b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1083, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(res - res_wo_q).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44ac9933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=128: int4=0.531 ms, fp16=0.226 ms, speedup=0.43x\n",
      "B=512: int4=1.299 ms, fp16=0.581 ms, speedup=0.45x\n",
      "B=2048: int4=4.806 ms, fp16=2.884 ms, speedup=0.60x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w_quant = w_quant.to('cuda')\n",
    "w_scale = w_scale.to('cuda')\n",
    "w = w.to('cuda').to(torch.float16)\n",
    "\n",
    "def bench(fn, iters=50, warmup=10):\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "        torch.cuda.synchronize()\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        start.record()\n",
    "        fn()\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "sizes = [128, 512, 2048]\n",
    "results = []\n",
    "for b in sizes:\n",
    "    x_cur = torch.randn(b, 2048, dtype=torch.float16, device='cuda')\n",
    "    t_int4 = bench(lambda: matmul_int4_fused(x_cur, w_quant, w_scale))\n",
    "    t_fp16 = bench(lambda: x_cur @ w.T)\n",
    "    results.append((b, t_int4, t_fp16))\n",
    "\n",
    "for b, t1, t2 in results:\n",
    "    print(f\"B={b}: int4={t1:.3f} ms, fp16={t2:.3f} ms, speedup={t2/t1:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9aec2",
   "metadata": {},
   "source": [
    "- оферхед на распаковку и деквантизацию"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
